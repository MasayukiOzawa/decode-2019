{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "pyspark3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "# NoteBook で PySpark を使用したデータ操作",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": " HDFS の「/clickstream_data」の CSV を読み込み、データフレームを作成する",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "results = spark.read\\\r\n            .option(\"inferSchema\", \"true\") \\\r\n            .option(\"header\", \"true\") \\\r\n            .csv('/clickstream_data') \\\r\n            .toDF(\r\n            \"wcs_click_date_sk\", \"wcs_click_time_sk\", \"wcs_sales_sk\", \"wcs_item_sk\", \"wcs_web_page_sk\", \"wcs_user_sk\"\r\n            )",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Starting Spark application\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1558243736434_0002</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"https://52.246.167.148:30443/gateway/default/yarn/proxy/application_1558243736434_0002/\">Link</a></td><td><a target=\"_blank\" href=\"https://52.246.167.148:30443/gateway/default/yarn/container/container_1558243736434_0002_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": "実行結果のデータ等の情報を出力",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "print(type(results))\r\n\r\nresults.printSchema()\r\nresults.show()",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "<class 'pyspark.sql.dataframe.DataFrame'>\nroot\n |-- wcs_click_date_sk: integer (nullable = true)\n |-- wcs_click_time_sk: integer (nullable = true)\n |-- wcs_sales_sk: string (nullable = true)\n |-- wcs_item_sk: integer (nullable = true)\n |-- wcs_web_page_sk: integer (nullable = true)\n |-- wcs_user_sk: string (nullable = true)\n\n+-----------------+-----------------+------------+-----------+---------------+-----------+\n|wcs_click_date_sk|wcs_click_time_sk|wcs_sales_sk|wcs_item_sk|wcs_web_page_sk|wcs_user_sk|\n+-----------------+-----------------+------------+-----------+---------------+-----------+\n|            38569|             4250|        null|       7840|             18|       null|\n|            38569|            85106|        null|      11130|             18|       null|\n|            38569|            52655|        null|       3716|             18|       null|\n|            38569|            70934|        null|      13243|             18|       null|\n|            38569|            40166|        null|       5389|             18|       null|\n|            38570|            73271|        null|       3331|             18|       null|\n|            38570|            24651|        null|      10049|             18|       null|\n|            38570|            23805|        null|        921|             18|       null|\n|            38570|            66458|        null|       4407|             18|       null|\n|            38570|            65912|        null|      11494|             18|       null|\n|            38570|            80073|        null|       1833|             18|       null|\n|            38570|             9522|        null|      17636|             18|       null|\n|            38570|            22779|        null|       9063|             18|       null|\n|            38570|            43276|        null|       8696|             18|       null|\n|            38570|            18021|        null|       3231|             18|       null|\n|            38570|            37963|        null|      17050|             18|       null|\n|            38570|            49125|        null|       9044|             18|       null|\n|            38570|            69824|        null|       3255|             18|       null|\n|            38570|            10028|        null|       1355|             18|       null|\n|            38570|             6749|        null|      16880|             18|       null|\n+-----------------+-----------------+------------+-----------+---------------+-----------+\nonly showing top 20 rows",
                    "output_type": "stream"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": "Hive テーブル作成前の設定 / 情報確認",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# SUCCESS ファイル保存の無効化\r\nsc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \r\n\r\n# Parquet ファイルが保存される、Warehouse ディレクトリを確認\r\nprint(spark.conf.get(\"spark.sql.warehouse.dir\"))",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "hdfs:///user/hive/warehouse",
                    "output_type": "stream"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": "実行結果を Parquet ファイルとして Hive テーブルに保存",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "results.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"web_clickstreams\")",
            "metadata": {},
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": "```\r\nkubectl exec -n mssql-cluster -it master-0 -c hadoop /bin/bash\r\nhive -e \"desc web_clickstreams\"\r\n```",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "Hive テーブルに対して、Spark SQL の実行",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "sqlDF = spark.sql(\"SELECT * FROM web_clickstreams LIMIT 10\")\r\nsqlDF.show()",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "+-----------------+-----------------+------------+-----------+---------------+-----------+\n|wcs_click_date_sk|wcs_click_time_sk|wcs_sales_sk|wcs_item_sk|wcs_web_page_sk|wcs_user_sk|\n+-----------------+-----------------+------------+-----------+---------------+-----------+\n|            38569|             4250|        null|       7840|             18|       null|\n|            38569|            85106|        null|      11130|             18|       null|\n|            38569|            52655|        null|       3716|             18|       null|\n|            38569|            70934|        null|      13243|             18|       null|\n|            38569|            40166|        null|       5389|             18|       null|\n|            38570|            73271|        null|       3331|             18|       null|\n|            38570|            24651|        null|      10049|             18|       null|\n|            38570|            23805|        null|        921|             18|       null|\n|            38570|            66458|        null|       4407|             18|       null|\n|            38570|            65912|        null|      11494|             18|       null|\n+-----------------+-----------------+------------+-----------+---------------+-----------+",
                    "output_type": "stream"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": "sqlDF = spark.sql(\"SELECT wcs_click_date_sk, COUNT(*) AS cnt\\\r\n                     FROM web_clickstreams\\\r\n                   GROUP BY wcs_click_date_sk\\\r\n                   ORDER BY COUNT(*) DESC\")\r\nsqlDF.show()",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "+-----------------+---+\n|wcs_click_date_sk|cnt|\n+-----------------+---+\n|            38577|108|\n|            38587| 93|\n|            38573| 93|\n|            38580| 84|\n|            38586| 81|\n|            38581| 81|\n|            38582| 81|\n|            38584| 78|\n|            38576| 78|\n|            38589| 72|\n|            38579| 72|\n|            38585| 66|\n|            38578| 66|\n|            38583| 63|\n|            38570| 63|\n|            38575| 63|\n|            38588| 63|\n|            38574| 60|\n|            38571| 42|\n|            38572| 42|\n+-----------------+---+\nonly showing top 20 rows",
                    "output_type": "stream"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": "データフレームの情報を確認",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "sqlDF.dtypes",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "[('wcs_click_date_sk', 'int'), ('cnt', 'bigint')]",
                    "output_type": "stream"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "code",
            "source": "type(sqlDF)",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "<class 'pyspark.sql.dataframe.DataFrame'>",
                    "output_type": "stream"
                }
            ],
            "execution_count": 10
        },
        {
            "cell_type": "markdown",
            "source": "データフレームの内容を CSV として出力",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "sqlDF.repartition(1)\\\r\n    .write \\\r\n    .format(\"csv\") \\\r\n    .option(\"header\",'true') \\\r\n    .mode(\"overwrite\") \\\r\n    .save(\"/output\")",
            "metadata": {},
            "outputs": [],
            "execution_count": 11
        }
    ]
}